{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Exercise: assigning reviewers to grants submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In this notebook we look at the problem of assigning reviewers to grants, using Dimensions as a knowledge base. Let us assume the following:\n",
    "\n",
    "* 100 grant applications (with title and abstract) are coming in \n",
    "* 20 researchers (identified with Dimensions Researcher ID) are panel members\n",
    "* The applications should be distributed among the panel members \n",
    "    * The final distribution is done manually\n",
    "    * We should inform the manual distribution with a **ranked order of researchers per grant application**, ideally with some scoring.\n",
    "    * Scoring does not have to be normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The goal is to produce a list of weighted researchers for each grant eg \n",
    "```\n",
    "{'grant-1' : [\n",
    "    (researcher-1, 0.9), (researcher-2, 0.8), (researcher-3, 0.5)\n",
    "]}\n",
    "```\n",
    "\n",
    "Hence the algorithm shoud be repeated for each grant and set of researchers coming in, and it breaks down as follows:\n",
    "\n",
    "* [1] for each grant\n",
    "    * [2] for each reviewer\n",
    "        * [3] score **relevance (Rel)** of grant against reviewer expertise\n",
    "        * [4] score **conflict of interest (CoI)** of grant against reviewer network\n",
    "        * save (reviewer, Rel score, CoI score) into candidates list \n",
    "    * sort candidates list by CoI score && Rel score\n",
    "\n",
    "Steps [1] and [2] are essentially a repetition of [3] and [4], so below we show the process only for 1 grant and 1 researcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For the purpose of this demo we are considering a sample grant from Dimensions: \n",
    "* [grant.2554006](https://app.dimensions.ai/details/grant/grant.2554006?search_text=A%20MULTI%27OMICS%20APPROACH%20TOWARDS%20DECIPHERING%20THE%20INFLUENCE%20OF%20THE%20MICROBIOME%20ON%20PRE&search_type=kws&search_field=full_search)\n",
    "\n",
    "And one scientist/reviewer which is working in a closely associated area to the previous grant (genetics/pediatrics):\n",
    "* [ur.01316535077.54](https://app.dimensions.ai/discover/publication?and_facet_researcher=ur.01316535077.54)\n",
    "\n",
    "Next, we store this data in variables and set up the Python libraries we will use to query the Dimensions API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dimensions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-424c25bb2424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDsl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pick a sample grant in genetics/pediatrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m sample_grant = dsl.query(\n\u001b[1;32m      5\u001b[0m     \"\"\"search grants where id=\"grant.2554006\" return grants[title+abstract]\"\"\")\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dimensions'"
     ]
    }
   ],
   "source": [
    "from dimensions import *\n",
    "dsl = Dsl('test')\n",
    "# pick a sample grant in genetics/pediatrics\n",
    "sample_grant = dsl.query(\n",
    "    \"\"\"search grants where id=\"grant.2554006\" return grants[title+abstract]\"\"\")\n",
    "TITLE, ABSTRACT = sample_grant['grants'][0]['title'], sample_grant['grants'][0]['abstract']\n",
    "REVIEWER = \"ur.01316535077.54\"\n",
    "CANDIDATES_LIST = [] # final list we want to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 1. Calculating the Relevance of a Grant to a Reviewer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We do this by \n",
    "* extracting key terms [K1] from the grant's title and description\n",
    "* extracting key terms [K2] from the reviewer's publications in last N years (divided by title and abstract) \n",
    "* getting the overlap between the two groups [K1~K2], and generating a single score from it\n",
    "\n",
    "NOTE: the 'terms' dsl api accepts only lowercase keywords, so we use a little helper function to deal with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# helper for terms extractor PS Adam: lowercase can be omitted here\n",
    "def cleanup(s):\n",
    "    return s.lower().replace(\"'\", \" \").replace(\"\\\"\", \" \")\n",
    "\n",
    "# terms extractor function (encoding required to prevent unicode errors with non standard chars)\n",
    "def dsl_terms(p):\n",
    "    q = \"\"\"extract_terms(\"%s\")\"\"\" % cleanup(p)\n",
    "    return dsl.query(q.encode(\"utf-8\"), show_result=False)['extracted_terms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1.1 Extract keywords from the grant title/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TITLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-164b6de03f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGRANT_TITLE_KEYWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsl_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTITLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mGRANT_DESC_KEYWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsl_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABSTRACT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GRANT_TITLE_KEYWORDS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRANT_TITLE_KEYWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GRANT_DESC_KEYWORDS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRANT_DESC_KEYWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TITLE' is not defined"
     ]
    }
   ],
   "source": [
    "GRANT_TITLE_KEYWORDS = dsl_terms(TITLE)\n",
    "GRANT_DESC_KEYWORDS = dsl_terms(ABSTRACT)\n",
    "print(\"GRANT_TITLE_KEYWORDS\", GRANT_TITLE_KEYWORDS)\n",
    "print(\"GRANT_DESC_KEYWORDS\", GRANT_DESC_KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1.2 Extract keywords from the reviewer's recent publications \n",
    "We extract keywords from the reviewer's publication list in the last 5 years. This is gonna take a little while so we print out a message to show the progress of the iteration as we go through all publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bf3aa116019c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"search publications where researchers.id=\"ur.01316535077.54\" and year>=2013 return publications[terms+id] limit 512\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "dsl.query(\"\"\"search publications where researchers.id=\"ur.01316535077.54\" and year>=2013 return publications[terms+id] limit 512\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9500467db361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pubs = dsl.query(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"search publications where researchers.id=\"%s\" and year>=2013 return publications[title+abstract] limit 512\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     % REVIEWER)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mREVIEWER_TITLE_KEYWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "pubs = dsl.query(\n",
    "    \"\"\"search publications where researchers.id=\"%s\" and year>=2013 return publications[title+abstract] limit 512\"\"\"\n",
    "    % REVIEWER)\n",
    "\n",
    "REVIEWER_TITLE_KEYWORDS = []\n",
    "REVIEWER_DESC_KEYWORDS = []\n",
    "c, tot = 0, len(pubs['publications'])\n",
    "for x in pubs['publications']:\n",
    "    c += 1\n",
    "    print(\"[%d / %d]\" % (c, tot), x['title'])\n",
    "    REVIEWER_TITLE_KEYWORDS += dsl_terms(x['title'])\n",
    "    REVIEWER_DESC_KEYWORDS += dsl_terms(x['abstract'])\n",
    "\n",
    "print(\"=========\")\n",
    "print(\"TOT REVIEWER_TITLE_KEYWORDS: \", len(REVIEWER_TITLE_KEYWORDS))\n",
    "print(\"TOT REVIEWER_DESC_KEYWORDS: \", len(REVIEWER_DESC_KEYWORDS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Note** If we don't want to split up title/desc keywords, we can simply query like this: `search publications where researchers.id=\"%s\" and year>=2013 return publications[terms] limit 1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf3aa116019c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"search publications where researchers.id=\"ur.01316535077.54\" and year>=2013 return publications[terms+id] limit 512\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "dsl.query(\"\"\"search publications where researchers.id=\"ur.01316535077.54\" and year>=2013 return publications[terms+id] limit 512\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Grouping by Frequency of keywords\n",
    "\n",
    "The same keyword can be found in more that one publication, so we want to take that as an indication that the reviewer is more closely associated to that topic. \n",
    "By grouping keywords this way we can end up with a dictionary where each keyword has a frequency of appearance, which can be used eventually to weight the relevancy score of a candidate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REVIEWER_TITLE_KEYWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ee02f461af26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mREVIEWER_TITLE_KEYWORDS_GROUPED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREVIEWER_TITLE_KEYWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mREVIEWER_DESC_KEYWORDS_GROUPED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREVIEWER_DESC_KEYWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REVIEWER_TITLE_KEYWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "REVIEWER_TITLE_KEYWORDS_GROUPED = Counter(REVIEWER_TITLE_KEYWORDS)\n",
    "REVIEWER_DESC_KEYWORDS_GROUPED = Counter(REVIEWER_DESC_KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now let's print these out as a table, using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= REVIEWER_TITLE_KEYWORDS BY MOST FREQUENT ========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'REVIEWER_TITLE_KEYWORDS_GROUPED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47f829c929ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# return empty DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========= REVIEWER_TITLE_KEYWORDS BY MOST FREQUENT ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREVIEWER_TITLE_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REVIEWER_TITLE_KEYWORDS_GROUPED' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# wrapper\n",
    "def build_df(a_dict):\n",
    "    if a_dict:\n",
    "        df = pd.DataFrame.from_dict(a_dict, orient='index').reset_index()\n",
    "        df = df.rename(columns={'index': 'term', 0: 'count'})\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No values to show\")\n",
    "        return pd.DataFrame() # return empty DF\n",
    "print(\"========= REVIEWER_TITLE_KEYWORDS BY MOST FREQUENT ========\")\n",
    "df = build_df(REVIEWER_TITLE_KEYWORDS_GROUPED)\n",
    "if not df.empty: \n",
    "    display(df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's do the same for the keywords extracted from the descriptions/abstracts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= REVIEWER_DESC_KEYWORDS BY MOST FREQUENT ========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'REVIEWER_DESC_KEYWORDS_GROUPED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75e87b0b0a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========= REVIEWER_DESC_KEYWORDS BY MOST FREQUENT ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREVIEWER_DESC_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REVIEWER_DESC_KEYWORDS_GROUPED' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"========= REVIEWER_DESC_KEYWORDS BY MOST FREQUENT ========\")\n",
    "df = build_df(REVIEWER_DESC_KEYWORDS_GROUPED)\n",
    "if not df.empty: display(df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1.3 Calculating the keywords overlap between the grant and the reviewer publications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is simply a matter of iterating through the two pair of keywords' lists, and highlighting the ones that appear in both places. \n",
    "\n",
    "**Note** we still deal with title and abstract/descriptions separately, based on the assumption that a match in the title is stronger that a match in the abstract. This will then be represented via a different weight multiplier for title matches, when calculating the final score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= OVERLAP_TITLE_KEYWORDS BY MOST FREQUENT ========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GRANT_TITLE_KEYWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-321ef65dea72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========= OVERLAP_TITLE_KEYWORDS BY MOST FREQUENT ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mOVERLAP_TITLE_KEYWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGRANT_TITLE_KEYWORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mREVIEWER_TITLE_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mOVERLAP_TITLE_KEYWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREVIEWER_TITLE_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRANT_TITLE_KEYWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"========= OVERLAP_TITLE_KEYWORDS BY MOST FREQUENT ========\")\n",
    "OVERLAP_TITLE_KEYWORDS = {}\n",
    "for t in GRANT_TITLE_KEYWORDS:\n",
    "    if t in REVIEWER_TITLE_KEYWORDS_GROUPED.keys():\n",
    "        OVERLAP_TITLE_KEYWORDS[t] = REVIEWER_TITLE_KEYWORDS_GROUPED[t]\n",
    "df = build_df(OVERLAP_TITLE_KEYWORDS)\n",
    "if not df.empty: display(df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Note** \n",
    "In this case the are no title keywords in common between the grant submissiona and the reviewer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= OVERLAP_DESC_KEYWORDS BY MOST FREQUENT ========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GRANT_DESC_KEYWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-38726f22a819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========= OVERLAP_DESC_KEYWORDS BY MOST FREQUENT ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mOVERLAP_DESC_KEYWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGRANT_DESC_KEYWORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mREVIEWER_DESC_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mOVERLAP_DESC_KEYWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREVIEWER_DESC_KEYWORDS_GROUPED\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRANT_DESC_KEYWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"========= OVERLAP_DESC_KEYWORDS BY MOST FREQUENT ========\")\n",
    "OVERLAP_DESC_KEYWORDS = {}\n",
    "for t in GRANT_DESC_KEYWORDS:\n",
    "    if t in REVIEWER_DESC_KEYWORDS_GROUPED.keys():\n",
    "        OVERLAP_DESC_KEYWORDS[t] = REVIEWER_DESC_KEYWORDS_GROUPED[t]\n",
    "df = build_df(OVERLAP_DESC_KEYWORDS)\n",
    "if not df.empty: display(df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally, we want to **reduce all of these results to a single number**, so that it'll be easier to then compare it with scores for other reviewers. \n",
    "\n",
    "This can be easily achieved in two steps:\n",
    "\n",
    "* sum up all counts for description keywords \n",
    "* sum up all counts for title keywords, applying a booster coefficient eg '2'. This is because matching title keywords can be more indicative of shared expertise.\n",
    "\n",
    "The sum of the two numbers above is the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total combined relevance score is:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CANDIDATES_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8199b8de058d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The total combined relevance score is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m CANDIDATES_LIST += [\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mREVIEWER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# other reviewers data would go here..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CANDIDATES_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "TITLE_WEIGHT = 2  # multiplier that gives title hits more weight\n",
    "\n",
    "score = (sum(OVERLAP_TITLE_KEYWORDS.values()) * TITLE_WEIGHT) + (sum(\n",
    "    OVERLAP_DESC_KEYWORDS.values()))\n",
    "\n",
    "print(\"The total combined relevance score is: \", score)\n",
    "\n",
    "CANDIDATES_LIST += [\n",
    "    [REVIEWER, score], \n",
    "    # other reviewers data would go here.. \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1.4 Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We have calculated the relevance of a *single* reviewer/researcher VS a submission by looking at the keywords-overlap of title and abstracts as distinct factors. \n",
    "\n",
    "We can easily **repeat the same process** for all the remaining N reviewers/researchers, simply by looping over a list of researcher IDs. \n",
    "\n",
    "The final scores can be added to the `CANDIDATES_LIST` variable, which can then be sorted before returning it to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CANDIDATES_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eefd23648810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCANDIDATES_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCANDIDATES_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCANDIDATES_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CANDIDATES_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "CANDIDATES_LIST = sorted(CANDIDATES_LIST, key=lambda x: x[1],  reverse=True)\n",
    "print(CANDIDATES_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1.5 Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A much faster but also much less transparent (and possibly less precise) approach would be to use a single query like this:\n",
    "\n",
    "```\n",
    "search publications in terms for \"..[1]..\" where researcher.id in [..[2]..] return researchers\n",
    "```\n",
    "\n",
    "Where: \n",
    "\n",
    "* [1] is a _list of terms_ obtained from extracting terms from the grant submission title+description\n",
    "* [2] is a _list of researchers_ i.e. the list of reviewers we are trying to rank\n",
    "\n",
    "This query essentially returns a researchers-facet on a list of publications filtered by terms and reviewers. The facet includes researchers that are not in the reviewers list, which can be skipped. The results are already ordered by relevance of the keywords. \n",
    "\n",
    "Main drawback is that the relevance score is not returned, nor it is clear how it is calculated. It would be useful to test these results on real world data and compare with the previous appraoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2. Calculating a Conflict of Interest score of a Reviewer VS a Grant Authors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Prerequisisite: \n",
    "\n",
    "* we must know the researcher ID of the authors of the grant submission\n",
    "\n",
    "Two aspects: \n",
    "\n",
    "* overlap between all coauthors of reviewer, and all authors of the grant \n",
    "    * extra: this could be extended by including grant author's co-authors, or second-degree co-authors etc..\n",
    "* overlap between all institutions of reviewer, and all institutions of the grants' authors \n",
    "    * extra: same considerations about second level objects\n",
    "    * extra: expand institutions by using full GRID hierarchy info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "RESEARCHERS = [\"ur.01331772327.01\"]  # the authors of the grant submission, one or more\n",
    "REVIEWER = \"ur.01316535077.54\" # the score is calculated against each single reviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2.1 Co-authors overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Note: the following query retrieves co-authors based on a publications list from the last 10 years. this This will make the query faster, but of course it can be changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c66fbf477661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m REVIEWER_PUBS_RESULTS = dsl.query(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"search publications where researchers.id=\"%s\" and year>=2008 return researchers limit 1000\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     % REVIEWER)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m REVIEWER_COAUTHORS = [\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "REVIEWER_PUBS_RESULTS = dsl.query(\n",
    "    \"\"\"search publications where researchers.id=\"%s\" and year>=2008 return researchers limit 1000\"\"\"\n",
    "    % REVIEWER)\n",
    "\n",
    "REVIEWER_COAUTHORS = [\n",
    "    x['id'] for x in REVIEWER_PUBS_RESULTS[\"researchers\"]\n",
    "    if x['id'] != REVIEWER\n",
    "]\n",
    "\n",
    "# find if there's an overlap between reviever's coauthors and the researchers\n",
    "overlap = [x for x in RESEARCHERS if x in REVIEWER_COAUTHORS]\n",
    "overlap_score = len(overlap)\n",
    "print(\"CO-AUTHORS CONFLICT OF INTEREST SCORE (0=no, 1+=yes): \\n=> \", overlap_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2.2 Organizations overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REVIEWER_PUBS_RESULTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-02f88a84f30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m REVIEWER_ORGANIZATIONS = next((item for item in REVIEWER_PUBS_RESULTS['researchers']\n\u001b[0m\u001b[1;32m      2\u001b[0m                       if item['id'] == REVIEWER), None)[\"research_orgs\"]\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRESEARCHERS_ORGANIZATIONS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mRESEARCHERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REVIEWER_PUBS_RESULTS' is not defined"
     ]
    }
   ],
   "source": [
    "REVIEWER_ORGANIZATIONS = next((item for item in REVIEWER_PUBS_RESULTS['researchers']\n",
    "                      if item['id'] == REVIEWER), None)[\"research_orgs\"]\n",
    "\n",
    "RESEARCHERS_ORGANIZATIONS = []\n",
    "for r in RESEARCHERS:\n",
    "    res = dsl.query(\n",
    "        \"\"\"search publications where researchers.id=\"%s\" return researchers limit 1\"\"\"\n",
    "        % r)\n",
    "    RESEARCHERS_ORGANIZATIONS += res[\"researchers\"][0][\"research_orgs\"]\n",
    "\n",
    "# find if there's an overlap between reviever's orgs and the researchers orgs\n",
    "overlap_institutions = [\n",
    "    x for x in RESEARCHERS_ORGANIZATIONS if x in REVIEWER_ORGANIZATIONS\n",
    "]\n",
    "overlap_institutions_score = len(overlap_institutions)\n",
    "print(\"ORGANIZATIONS CONFLICT OF INTEREST SCORE (0=no, 1+=yes): \\n=> \", overlap_institutions_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2.3 Wrapping up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The steps above must be repeated for each grant submission - reviewer pair.\n",
    "\n",
    "Eventually the scores can be combined within the `CANDIDATES_LIST` result (for a given grant submission) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CANDIDATES_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d29c37959866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCANDIDATES_LIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREVIEWER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moverlap_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_institutions_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCANDIDATES_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CANDIDATES_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "for c in CANDIDATES_LIST:\n",
    "    if c[0] == REVIEWER:\n",
    "        c += [overlap_score, overlap_institutions_score]\n",
    "print(CANDIDATES_LIST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
