{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "xjqXbj55dkH0"
   },
   "source": [
    "# Dimensions Statistics Report, for one or more Organizations\n",
    "\n",
    "This notebook allows to produce up-to-date statistics on the number of documents associated to one of more GRID organizations. These numbers can be used to then generate visual summaries like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1576698178269,
     "user": {
      "displayName": "Michele Pasin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu8LVjIGgontF2Wax51BoL5KFx8esezX3bUmaa0g=s64",
      "userId": "10309320684375994511"
     },
     "user_tz": 0
    },
    "id": "RthnvlgabJy5",
    "outputId": "1eb96926-6bc3-4d90-ac64-06c045762ed0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://api-sample-data.dimensions.ai/stats-notebook/chart-example-1.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://api-sample-data.dimensions.ai/stats-notebook/chart-example-1.jpg\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1576698177015,
     "user": {
      "displayName": "Michele Pasin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu8LVjIGgontF2Wax51BoL5KFx8esezX3bUmaa0g=s64",
      "userId": "10309320684375994511"
     },
     "user_tz": 0
    },
    "id": "_4oKiYqbb0g-",
    "outputId": "b70832de-51a0-4673-d9fd-8e4347a047a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://api-sample-data.dimensions.ai/stats-notebook/chart-example-2.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://api-sample-data.dimensions.ai/stats-notebook/chart-example-2.jpg\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "nqHEbQcDbIlt"
   },
   "source": [
    "### Statistics Detailed Description\n",
    "\n",
    "The statistics are divided into two groups and two corresponding CSV files:\n",
    "\n",
    "**1) Number of Documents Overview** - saved to `overview_objects.csv`\n",
    "\n",
    "- Publications number from the GRID IDs.\n",
    "  - Query template: `search publications where research_orgs = []..`\n",
    "- Grant number and funding given to the GRID IDs (funding aggregated in US dollars).\n",
    "  - Query template: `search grants where research_orgs = []..`\n",
    "- Patents with GRID IDs as assignees. \n",
    "  - Query template: `search patents where assignees = []..`\n",
    "- Clinical Trials with associated GRID IDs.\n",
    "  - Query template: `search clinical_trials where organizations = []..`\n",
    "- Policy papers published by GRID IDs (**NOTE** policy papers will always need to adjusted manually since it is tricky)\n",
    "  - Query template: `search policy_documents where publisher_org = []..`\n",
    "\n",
    "\n",
    "**2) Incoming Links Overview (= publication citations)** - saved to `overview_links.csv`\n",
    "\n",
    "- Publication count citing the GRID ID publications. \n",
    "  - Query template: `search publications where research_orgs=[] return year aggregate citations_total` (then the yearly citations are summed up).\n",
    "- Clinical trials referencing the publications of the GRID ID.\n",
    "  - Query template: `search clinical_trials where publication_ids = [..]`\n",
    "- Grants linked to publications of GRID ID.\n",
    "  - Query template: `search grants where resulting_publications_ids ..`\n",
    "- Patents referencing the GRID ID publications. \n",
    "  - Query template: `search patents where publication_ids in`\n",
    "- Policy documents referencing the GRID ID publications.\n",
    "  - Query template: `search policy_document where publication_ids in`\n",
    "\n",
    "\n",
    "**NOTE** \n",
    "\n",
    "If the starting GRID IDs generate a dataset of more than 50k publications, only the __most recent 50k publications__  will be extracted. \n",
    "\n",
    "This means that in such cases the group 2 statistics should then be read as the *number of patents citing the most recent 50k publications* etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "JmIHSwq46aK0"
   },
   "source": [
    "# 1. Install Libraries and Log into Dimensions API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "5cbCB86W0EHw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==\n",
      "Installing libraries..\n",
      "==\n",
      "Logging in..\n",
      "Environment: Jupyter Notebook\n",
      "... looking for API credentials file\n",
      "DimCli v0.6.6.5 - Succesfully connected to <https://app.dimensions.ai> (method: dsl.ini file)\n"
     ]
    }
   ],
   "source": [
    "# @markdown **Privacy tip**: leave the password blank and you'll be asked for it later. This can be handy on shared computers.\n",
    "username = \"\"  #@param {type: \"string\"}\n",
    "password = \"\"  #@param {type: \"string\"}\n",
    "endpoint = \"https://app.dimensions.ai\"  #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "# INSTALL/LOAD LIBRARIES \n",
    "# ps optimized for Google Colab /modify installation as needed based on your environment\n",
    "# \n",
    "print(\"==\\nInstalling libraries..\")\n",
    "!pip install dimcli networkx pyvis -U --quiet \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm.notebook import tqdm as progressbar\n",
    "import dimcli \n",
    "from dimcli.shortcuts import *\n",
    "\n",
    "# AUTHENTICATION \n",
    "# https://github.com/digital-science/dimcli#authentication\n",
    "#\n",
    "# == Google Colab users ==\n",
    "# If username/password not provided, the interactive setup assistant `dimcli --init` is invoked\n",
    "#\n",
    "# == Jupyter Notebook users == \n",
    "# If username/password not provided, try to use the global API credentials file.\n",
    "# To create one, open a terminal (File/New/Terminal) and run `dimcli --init` from there\n",
    "#  \n",
    "#\n",
    "print(\"==\\nLogging in..\")\n",
    "if username and password:\n",
    "  dimcli.login(username, password, endpoint)\n",
    "else:\n",
    "  if 'google.colab' in sys.modules:\n",
    "    print(\"Environment: Google Colab\")\n",
    "    if username and not password:\n",
    "      import getpass\n",
    "      password = getpass.getpass(prompt='Password: ')     \n",
    "      dimcli.login(username, password, endpoint)\n",
    "    else:\n",
    "      print(\"... launching interactive setup assistant\")\n",
    "      !dimcli --init    \n",
    "      dimcli.login()\n",
    "  else:\n",
    "    print(\"Environment: Jupyter Notebook\\n... looking for API credentials file\")\n",
    "    dimcli.login()\n",
    "\n",
    "dsl = dimcli.Dsl()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "cVhIN498fi9b"
   },
   "source": [
    "# 2. Enter GRIDs and start data extraction\n",
    "\n",
    "Tip: pick one from https://grid.ac/institutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Zvvob8TtHANv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRIDs entered (unique):  2 \n",
      " => ['grid.474488.3', 'grid.473100.1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "---<br /><a href=\"https://app.dimensions.ai/discover/publication?or_facet_research_org=grid.474488.3&or_facet_research_org=grid.473100.1\">View selected organizations in Dimensions &#x29c9;</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==\n",
      "Created data folder: stats_data_grid.474488.3/\n",
      "\n",
      "\n",
      "======\n",
      "======\n",
      "Starting data extraction part [1]....\n",
      "======\n",
      "======\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michele.pasin/Envs/jupyterlab/lib/python3.7/site-packages/ipykernel_launcher.py:79: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ffbdbe059e4d74b47c64452a407df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Publications: 1 (total = 100)\n",
      "Returned Grants: 0\n",
      "Grants total Funding:  0\n",
      "Returned Patents: 1 (total = 106)\n",
      "Returned Clinical_trials: 0\n",
      "WARNINGS [1]\n",
      "Field 'organizations' is deprecated in favor of research_orgs. Please refer to https://docs.dimensions.ai/dsl/releasenotes.html for more details\n",
      "Returned Policy_documents: 0\n",
      "\n",
      "===\n",
      "Saved:  stats_data_grid.474488.3/overview_objects.csv\n",
      "\n",
      "\n",
      "======\n",
      "======\n",
      "Starting data extraction part [2]....\n",
      "======\n",
      "======\n",
      "\n",
      "\n",
      "===\n",
      "Extracting all publications for GRIDs (max 50k, sorted by date)\n",
      "100 / 100\n",
      "===\n",
      "Extracting publications citations\n",
      "Returned Year: 20\n",
      "===\n",
      "Total Citations:  591.0\n",
      "===\n",
      "Extracting grants citations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michele.pasin/Envs/jupyterlab/lib/python3.7/site-packages/ipykernel_launcher.py:154: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858df9ea1f714308b8815fddc693f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total related grants found:  15\n",
      "===\n",
      "Extracting patents citations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michele.pasin/Envs/jupyterlab/lib/python3.7/site-packages/ipykernel_launcher.py:182: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb4fefcc804bc4b0a4aaf3df9ee050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total related patents found:  7\n",
      "===\n",
      "Extracting clinical_trials citations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michele.pasin/Envs/jupyterlab/lib/python3.7/site-packages/ipykernel_launcher.py:209: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3591a1368f1a4ad5a58b5a10abafa773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total related clinical_trials found:  0\n",
      "===\n",
      "Extracting policy_documents citations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michele.pasin/Envs/jupyterlab/lib/python3.7/site-packages/ipykernel_launcher.py:237: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af603e187df84fa0bc211d15d5304a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total related policy_documents found:  0\n",
      "===\n",
      "Generating summary...\n",
      "===\n",
      "Saved:  stats_data_grid.474488.3/overview_links.csv\n",
      "\n",
      "===\n",
      "Downloading...\n",
      "ERROR - Google Colab couldn't download - please try again...\n"
     ]
    }
   ],
   "source": [
    "#@markdown Please enter up to 30 GRID IDs, comma separated.\n",
    "\n",
    "organizations = \"grid.473100.1, grid.474488.3\" #@param {type: \"string\"}\n",
    "gridids = [x.strip() for x in organizations.split(\",\")]\n",
    "gridids = list(set(gridids))\n",
    "\n",
    "MAX_GRIDS = 30\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "def dimensions_url(grids):\n",
    "    root = \"https://app.dimensions.ai/discover/publication?or_facet_research_org=\"\n",
    "    return root + \"&or_facet_research_org=\".join([x for x in grids])\n",
    "\n",
    "\n",
    "if len(gridids) > MAX_GRIDS:\n",
    "  print(\"You entered too many GRID IDs. Max is \", MAX_GRIDS)\n",
    "  raise\n",
    "else:\n",
    "  print(\"GRIDs entered (unique): \", len(gridids), \"\\n =>\", gridids)\n",
    "  # gen link to Dimensions\n",
    "  display(HTML('---<br /><a href=\"{}\">View selected organizations in Dimensions &#x29c9;</a>'.format(dimensions_url(gridids))))\n",
    "\n",
    "#\n",
    "# data-saving utils \n",
    "#\n",
    "\n",
    "DATAFOLDER = \"stats_data_\" + str(gridids[0])\n",
    "if not os.path.exists(DATAFOLDER):\n",
    "  !mkdir $DATAFOLDER\n",
    "  print(f\"==\\nCreated data folder:\", DATAFOLDER + \"/\")\n",
    "#\n",
    "#\n",
    "def save_as_csv(df, save_name_without_extension):\n",
    "    \"usage: `save_as_csv(dataframe, 'filename')`\"\n",
    "    df.to_csv(f\"{DATAFOLDER}/{save_name_without_extension}.csv\", index=False)\n",
    "    print(\"===\\nSaved: \", f\"{DATAFOLDER}/{save_name_without_extension}.csv\")\n",
    "\n",
    "\n",
    "###\n",
    "#\n",
    "# STEP 1: Basic stats: Publications, Grants, Patents, Clinical Trials etc..\n",
    "# In this step we count how many documents direcly associated to the GRID IDs exist - for each of the main Dimensions document types.\n",
    "#\n",
    "###\n",
    "\n",
    "\n",
    "print(\"\\n\\n======\\n======\\nStarting data extraction part [1]....\\n======\\n======\\n\\n\")\n",
    "\n",
    "from tqdm import tqdm_notebook as pbar\n",
    "\n",
    "query_templates = {\n",
    "    \"publications\" : \"\"\"search publications where research_orgs in {} return publications[id] {}\"\"\" ,\n",
    "    \"grants\" : \"\"\"search grants where research_orgs in {} return grants[id] {}\"\"\" ,\n",
    "    \"patents\" : \"\"\"search patents where assignees in {} return patents[id] {}\"\"\" ,\n",
    "    \"clinical_trials\" : \"\"\"search clinical_trials where organizations in {} return clinical_trials[id] {}\"\"\" ,\n",
    "    \"policy_documents\" : \"\"\"search policy_documents where publisher_org in {} return policy_documents[id] {}\"\"\" ,\n",
    "}\n",
    "\n",
    "VERBOSE = True\n",
    "# CHUNKS_SIZE = 30 # how many grids to process at a time\n",
    "results = []\n",
    "\n",
    "\n",
    "def get_funding():\n",
    "  \"\"\"Extra query to get funding\n",
    "  We're using `title_language` to facet + aggregate as that seems to be field which is a)single and unique per grant and b) always present; \n",
    "  Tried using also `start_year` but often grants were missing cause they didn't have one.\n",
    "  \"\"\"\n",
    "  q = \"\"\"search grants where research_orgs in {} return title_language aggregate funding limit 500\"\"\".format(json.dumps(gridids))\n",
    "  d = dsl.query(q, verbose=False)  \n",
    "  funding_total = 0\n",
    "  if d.count_total and \"title_language\" in d.json:\n",
    "    for x in d.title_language:\n",
    "      funding_total += x['funding']\n",
    "  print(\"Grants total Funding: \", funding_total)\n",
    "  return funding_total\n",
    "\n",
    "\n",
    "loop1 = pbar(list(query_templates))\n",
    "for doctype in loop1:\n",
    "  loop1.set_description(\"Extracting Total Count of %s\" % doctype.capitalize())\n",
    "  # dsl query\n",
    "  q = query_templates[doctype].format(json.dumps(gridids), \"limit 1\")\n",
    "  data = dsl.query(q, verbose=VERBOSE)\n",
    "  if doctype == \"grants\":\n",
    "    money = get_funding()\n",
    "    results.append({\"source\" : doctype, \"documents\" : data.count_total, \"funding\" : money})\n",
    "  else:\n",
    "    results.append({\"source\" : doctype, \"documents\" : data.count_total, \"funding\" : 0})\n",
    "  time.sleep(1)\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# save to a dataframe\n",
    "summary = pd.DataFrame().from_dict(results)\n",
    "save_as_csv(summary, \"overview_objects\")\n",
    "# summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "#\n",
    "# STEP 2: References: from Grants, Clinical Trials, Patents etc... to Publications\n",
    "# In this section we extract existing links from the various document types to the publications from the selected GRID IDs.\n",
    "#\n",
    "###\n",
    "\n",
    "\n",
    "print(\"\\n\\n======\\n======\\nStarting data extraction part [2]....\\n======\\n======\\n\\n\")\n",
    "\n",
    "#\n",
    "# 0. prerequisite: pubs baseset\n",
    "#\n",
    "\n",
    "print(\"===\\nExtracting all publications for GRIDs (max 50k, sorted by date)\")\n",
    "\n",
    "q = \"\"\"search publications where research_orgs in {} return publications[id] sort by date\"\"\"\n",
    "pubs = dsl.query_iterative(q.format(json.dumps(gridids)))\n",
    "pubs = pubs.as_dataframe()\n",
    "pubsids = list(pubs['id'])\n",
    "\n",
    "# dict to store final results\n",
    "overview_links = { 'publications' : 0, 'grants' : 0, 'patents' : 0, 'clinical_trials' : 0, 'policy_documents' : 0 }\n",
    "\n",
    "\n",
    "#\n",
    "# 1. pubs references\n",
    "#\n",
    "print(\"===\\nExtracting publications citations\")\n",
    "\n",
    "q = \"\"\"search publications where research_orgs in {} return year aggregate citations_total limit 1000 \"\"\"\n",
    "d = dsl.query(q.format(json.dumps(gridids)))\n",
    "citations_total = 0\n",
    "if d.count_total and \"year\" in d.json:\n",
    "  for x in d.year:\n",
    "    citations_total += x['citations_total']\n",
    "print(\"===\\nTotal Citations: \", citations_total)\n",
    "overview_links['publications'] = citations_total\n",
    "\n",
    "#\n",
    "# 2. grants references \n",
    "#\n",
    "\n",
    "print(\"===\\nExtracting grants citations\")\n",
    "\n",
    "q = \"\"\"search grants where resulting_publication_ids in {} return grants[id]\"\"\"\n",
    "\n",
    "# iterate pubids using chunks \n",
    "VERBOSE = False\n",
    "CHUNKS_SIZE = 300 \n",
    "results = []\n",
    "\n",
    "for chunk in pbar(list(chunks_of(pubsids, CHUNKS_SIZE))):\n",
    "  query = q.format(json.dumps(chunk))\n",
    "  data = dsl.query_iterative(query, verbose=VERBOSE)\n",
    "  results += data.grants\n",
    "  time.sleep(0.5)\n",
    "\n",
    "#\n",
    "# put the citing grants data into a dataframe, remove duplicates and save\n",
    "grants = pd.DataFrame().from_dict(results)\n",
    "# print(\"===\\nRelated grants found: \", len(grants))\n",
    "grants.drop_duplicates(subset='id', inplace=True)\n",
    "print(\"Total related grants found: \", len(grants))\n",
    "overview_links['grants'] = len(grants)\n",
    "\n",
    "\n",
    "#\n",
    "# 3. patents references \n",
    "#\n",
    "\n",
    "print(\"===\\nExtracting patents citations\")\n",
    "\n",
    "q = \"\"\"search patents where publication_ids in {} return patents[id]\"\"\"\n",
    "\n",
    "# iterate pubids using chunks \n",
    "VERBOSE = False\n",
    "CHUNKS_SIZE = 300 \n",
    "results = []\n",
    "\n",
    "for chunk in pbar(list(chunks_of(pubsids, CHUNKS_SIZE))):\n",
    "  query = q.format(json.dumps(chunk))\n",
    "  data = dsl.query_iterative(query, verbose=VERBOSE)\n",
    "  results += data.patents\n",
    "  time.sleep(0.5)\n",
    "\n",
    "#\n",
    "# put the citing grants data into a dataframe, remove duplicates and save\n",
    "patents = pd.DataFrame().from_dict(results)\n",
    "# print(\"===\\nRelated grants found: \", len(grants))\n",
    "patents.drop_duplicates(subset='id', inplace=True)\n",
    "print(\"Total related patents found: \", len(patents))\n",
    "overview_links['patents'] = len(patents)\n",
    "\n",
    "#\n",
    "# 4. clinical trials references \n",
    "#\n",
    "\n",
    "print(\"===\\nExtracting clinical_trials citations\")\n",
    "\n",
    "q = \"\"\"search clinical_trials where publication_ids in {} return clinical_trials[id]\"\"\"\n",
    "\n",
    "# iterate pubids using chunks \n",
    "VERBOSE = False\n",
    "CHUNKS_SIZE = 300 \n",
    "results = []\n",
    "\n",
    "for chunk in pbar(list(chunks_of(pubsids, CHUNKS_SIZE))):\n",
    "  query = q.format(json.dumps(chunk))\n",
    "  data = dsl.query_iterative(query, verbose=VERBOSE)\n",
    "  results += data.clinical_trials\n",
    "  time.sleep(0.5)\n",
    "\n",
    "#\n",
    "# put the citing grants data into a dataframe, remove duplicates and save\n",
    "clinical_trials = pd.DataFrame().from_dict(results)\n",
    "# print(\"===\\nRelated grants found: \", len(grants))\n",
    "clinical_trials.drop_duplicates(subset='id', inplace=True)\n",
    "print(\"Total related clinical_trials found: \", len(clinical_trials))\n",
    "overview_links['clinical_trials'] = len(clinical_trials)\n",
    "\n",
    "\n",
    "#\n",
    "# 5. policy documents references \n",
    "#\n",
    "\n",
    "print(\"===\\nExtracting policy_documents citations\")\n",
    "\n",
    "q = \"\"\"search policy_documents where publication_ids in {} return policy_documents[id]\"\"\"\n",
    "\n",
    "# iterate pubids using chunks \n",
    "VERBOSE = False\n",
    "CHUNKS_SIZE = 300 \n",
    "results = []\n",
    "\n",
    "for chunk in pbar(list(chunks_of(pubsids, CHUNKS_SIZE))):\n",
    "  query = q.format(json.dumps(chunk))\n",
    "  data = dsl.query_iterative(query, verbose=VERBOSE)\n",
    "  results += data.policy_documents\n",
    "  time.sleep(0.5)\n",
    "\n",
    "#\n",
    "# put the citing grants data into a dataframe, remove duplicates and save\n",
    "policy_documents = pd.DataFrame().from_dict(results)\n",
    "# print(\"===\\nRelated grants found: \", len(grants))\n",
    "policy_documents.drop_duplicates(subset='id', inplace=True)\n",
    "print(\"Total related policy_documents found: \", len(policy_documents))\n",
    "overview_links['policy_documents'] = len(policy_documents)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# 6. collate results and save \n",
    "#\n",
    "\n",
    "\n",
    "print(\"===\\nGenerating summary...\")\n",
    "\n",
    "# we want to produce a dict list this \n",
    "# overview_links = [\n",
    "#                   {'from_source' : 'publications',     'to_source' : 'publications', 'links' : 0 },\n",
    "#                   {'from_source' : 'grants',           'to_source' : 'publications', 'links' : 0 },\n",
    "#                   {'from_source' : 'patents',          'to_source' : 'publications', 'links' : 0 },\n",
    "#                   {'from_source' : 'clinical_trials',  'to_source' : 'publications', 'links' : 0 },\n",
    "#                   {'from_source' : 'policy_documents', 'to_source' : 'publications', 'links' : 0 },\n",
    "#                 ]\n",
    "\n",
    "nice_links = []\n",
    "for x in overview_links:\n",
    "  nice_links.append({'from_source' : x, 'to_source' : 'publications', 'links' : overview_links[x] })\n",
    "\n",
    "#\n",
    "# finally, save to a dataframe\n",
    "df2 = pd.DataFrame().from_dict(nice_links)\n",
    "#\n",
    "save_as_csv(df2, f\"overview_links\")\n",
    "#\n",
    "df2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "#\n",
    "# STEP 3: Download\n",
    "#\n",
    "###\n",
    "\n",
    "print(\"\\n===\\nDownloading...\")\n",
    "\n",
    "# zip up all files to make download easier\n",
    "import zipfile\n",
    "import os \n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "zip_name = DATAFOLDER + '.zip'\n",
    "zipf = zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir(DATAFOLDER + '/', zipf)\n",
    "zipf.close()\n",
    "\n",
    "try:\n",
    "  # try to download from colab: sometimes it fails hence print a message\n",
    "  from google.colab import files\n",
    "  time.sleep(5)\n",
    "  files.download(zip_name) \n",
    "  print(\"\\n===\\nDone.\")\n",
    "except:\n",
    "  print(\"ERROR - Google Colab couldn't download - please try again...\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4XWopF8elGKt"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7WjK2RZRQc9Y"
   },
   "source": [
    "The exports should download automatically. \n",
    "However if there's an error with downloading the file, try running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "fceJrycWQbVs"
   },
   "outputs": [],
   "source": [
    "files.download(zip_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "GjjB66e9Zl8E"
   },
   "source": [
    "## Network diagram \n",
    "\n",
    "Run the following cell to build a simple network diagram based on the data extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1576698254758,
     "user": {
      "displayName": "Michele Pasin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBu8LVjIGgontF2Wax51BoL5KFx8esezX3bUmaa0g=s64",
      "userId": "10309320684375994511"
     },
     "user_tz": 0
    },
    "id": "VIOef-sJZOcM",
    "outputId": "ef928fb5-8fe9-4678-8dec-74baaf21a8fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"945px\"\n",
       "            height=\"500px\"\n",
       "            src=\"stats_data_grid.474488.3/graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11e511d90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Please select the width of the diagram\n",
    "\n",
    "WIDTH = 945 #@param {type: \"slider\", min: 500, max: 1200}\n",
    "\n",
    "\n",
    "#\n",
    "# 7. Display Network Graph \n",
    "#\n",
    "\n",
    "from dimcli.core.extras import NetworkViz\n",
    "json_summary = json.loads(summary.to_json(orient=\"records\"))\n",
    "json_links = json.loads(df2.to_json(orient=\"records\"))\n",
    "\n",
    "labels = [x['source'] + \" ({})\".format(x['documents']) for x in json_summary]\n",
    "titles = [str(x['documents']) + \" \" + x['source'] + \" from {}\".format(str(gridids)) for x in json_summary]\n",
    "\n",
    "g = NetworkViz(notebook=True, width=\"{}px\".format(WIDTH))\n",
    "g.add_nodes(list(summary.source),\n",
    "            value=[x for x in summary.documents], # never 0, default 1\n",
    "            title=titles,\n",
    "            label=labels,\n",
    "            color=[\"#00ff1e\", \"#162347\", \"#dd4b39\", \"#00bfff\", \"#ffbf00\"])\n",
    "\n",
    "for x in json_links:\n",
    "  g.add_edge(x['from_source'], x['to_source'], \n",
    "             value=int(x['links']), \n",
    "             label=int(x['links']), \n",
    "             arrows=\"to\",\n",
    "             title=\"{} {} link to {} publications \".format(int(x['links']), x['from_source'], str(gridids)))\n",
    "\n",
    "g.show(DATAFOLDER + \"/graph.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dimensions-Statistics-for-any-org-v2-PROD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
